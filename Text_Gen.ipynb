{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies"
      ],
      "metadata": {
        "id": "t8wauhQAq0V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gradio as gr\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "N6YEuRxVp97S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading fine-tuned model"
      ],
      "metadata": {
        "id": "sS8W-DSRqr0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment lines below and input path of model if you want to use a fine-tuned model\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# model = torch.load('path', map_location ='cpu')"
      ],
      "metadata": {
        "id": "GuAPZ4xfqMzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Pre-trained model"
      ],
      "metadata": {
        "id": "TLhRnAP2qyAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "rE--7WlVqla4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation function"
      ],
      "metadata": {
        "id": "bbcucBZbrNGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7zHOW5Ip52d"
      },
      "outputs": [],
      "source": [
        "#Inputs: prompt:string, sentence_length:int, num_sequences:int\n",
        "#Outputs: generated_text:string\n",
        "def generate_text(prompt, sentence_length, num_sequences):\n",
        "    \n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    \n",
        "    # Generate text\n",
        "    generated_text = model.generate(\n",
        "        input_ids,\n",
        "        max_length=sentence_length * num_sequences,\n",
        "        min_new_tokens = 7,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "#Inputs: generated_text:string\n",
        "#Outputs: filtered_sentences:array of strings\n",
        "def split_by_sentences(generated_text):\n",
        "    # Split text by '.' character\n",
        "    sentences = sent_tokenize(generated_text)\n",
        "    # Remove incomplete sentences\n",
        "    filtered_sentences = [sentence for sentence in sentences if sentence.endswith('.')]\n",
        "    # Convert array of sentences into string and add newline\n",
        "    filtered_sentences = '\\n'.join([sentence for sentence in filtered_sentences])\n",
        "    return filtered_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio Interface"
      ],
      "metadata": {
        "id": "zjFvIOChrSV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = 'This is a text generation model which uses the transformer architecture to generate text given an prompt'\n",
        "title = 'Story Generator'\n",
        "\n",
        "text_gen_UI = gr.Interface(generate_text,\n",
        "                  inputs=['text', gr.inputs.Slider(0,100, label='Number of generated words'), gr.inputs.Slider(0,20, label='Number of sequences')], \n",
        "                  outputs=[gr.outputs.Textbox()])\n",
        "text_clean_UI = gr.Interface(split_by_sentences,\n",
        "                  inputs=['text'], \n",
        "                  outputs=[gr.outputs.Textbox()])\n",
        "\n",
        "gr.Series(text_gen_UI, text_clean_UI, description=description, title=title).launch()"
      ],
      "metadata": {
        "id": "iMxEEzw_rUoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}